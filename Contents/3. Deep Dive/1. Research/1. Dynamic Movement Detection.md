# Can dynamic movements be detected with hand tracking in VR?
In secrets of Igancios I have created a system that can already recognise a sequence of gestures a user performs. These gestures are all static hand poses. I want to find out if I can recognise movement to, in this case, cast spells. Since my group got feedback to maybe use movements to change the way a spell works, or use our current cast gesture and instead of just performing the gesture you would actually have to push it forward for it to work.

## Currently available options
I looked at possible existing options. I started by looking for games that used hand tracking, but I was only able to find hand tracking physics games, they did not make use of any gesture recognition. 
Some games I found were: [Elixer](https://www.meta.com/nl-nl/experiences/elixir/3793077684043441/) and [Hand Physics Lab](https://www.meta.com/nl-nl/experiences/hand-physics-lab/3392175350802835/).

After this I looked if there were other people facing the same challenge as me. I was able to find one other on [unity's subreddit](https://www.reddit.com/r/Unity3D/comments/1bncp4f/recognizing_dynamic_hand_gestures_with_unitys_xr/). This person was also trying to recognise dynamic hand gestures with the XR Hands package. Sadly this post didn't gain any traction and got no responses. However the OP did comment that he was able to solve the problem with the [P Dollar unity package](https://assetstore.unity.com/packages/tools/input-management/pdollar-point-cloud-gesture-recognizer-21660). Since this was already something I had thought of using, it confirmed that I would be able to use this package to create some sort of dynamic gesture recognition.

From this post I also learned about [AnyGesture](https://www.mdpi.com/2076-3417/12/4/1888), which is exactly what I'm trying to make. Sadly I was only able to find documentation on this and no way to implement it, but if there is a way this seems like it would be the best way to implement dynamic hand gesture recognition.

Lastly I looked at [Meta's hand tracking package documentation](https://developers.meta.com/horizon/documentation/unity/unity-isdk-hand-pose-detection#velocityrecognition), since I thought I read it had dynamic gesture recognition. This package is able to check the velocity of joints, this is similair to an idea I had to create my own dynamic Gesture recognition. If you donn't use the XR Hands package, I would recommend trying this approach.

## My implementation
I had multiple ways to achieve this in mind. One way is to use the position of the hand and set some kind of target position before a gesture counts ([more on that can be read here](../2.%20Devlogs/01.%20Forward%20Motion.md)) and the other was to use the [P Dollar package](https://assetstore.unity.com/packages/tools/input-management/pdollar-point-cloud-gesture-recognizer-21660) and recreate the AnyGesture program with this package ([more on that can be read here](../2.%20Devlogs/2.%202D%20Dynamic%20Gestures.md)). These two options both had their drawbacks as can be read in the devlogs I wrote about them, but the P Dollar version seemed to work the best.

## Advice
If you are trying to create dynamic gesture recognition from scratch, I would recommend to use the P dollar package, or at least use AnyGesture as an example, I believe that the only drawback for this system might be that it doesn't track the speed at which the player performs the gestures. But this could be added later on. If ou want something that you can just implement and use immediately, the Meta SDK seems the way to go. It doesn't seem as advanced as the AnyGesture system, but it does check the joint velocity, which should make it possible to set certain speed thresholds.

## Conclusion
In conclusion, detecting dynamic movements with hand tracking in VR is a challenging but achievable goal. Through research and experimentation, I found that existing options like the P Dollar package and Meta's hand tracking SDK offer viable solutions. The P Dollar package allows for flexible dynamic gesture recognition, while the Meta SDK provides velocity tracking, which can be used to create motion-based interactions.

## Sources
[XR Interaction and Input. (z.d.). Productboard. Geraadpleegd op 18 november 2024, van https://portal.productboard.com/unity/4-unity-platform-ar-vr/tabs/8-xr-interaction-and-input](https://portal.productboard.com/unity/4-unity-platform-ar-vr/tabs/8-xr-interaction-and-input)

[Fofosomesome. (2024, maart). Recognizing Dynamic Hand Gestures with Unity’s XR Hand Subsystem. Reddit.com. Geraadpleegd op 18 november 2024, van https://www.reddit.com/r/Unity3D/comments/1bncp4f/recognizing_dynamic_hand_gestures_with_unitys_xr/](https://www.reddit.com/r/Unity3D/comments/1bncp4f/recognizing_dynamic_hand_gestures_with_unitys_xr/)

[PDollar Point-Cloud Gesture Recognizer | Input Management | Unity Asset Store. (2018b, maart 8). Unity Asset Store. https://assetstore.unity.com/packages/tools/input-management/pdollar-point-cloud-gesture-recognizer-21660](https://assetstore.unity.com/packages/tools/input-management/pdollar-point-cloud-gesture-recognizer-21660)

[Meta.com. https://www.meta.com/nl-nl/experiences/elixir/3793077684043441/](https://www.meta.com/nl-nl/experiences/elixir/3793077684043441/)

[Meta.com. https://www.meta.com/nl-nl/experiences/hand-physics-lab/3392175350802835/](https://www.meta.com/nl-nl/experiences/hand-physics-lab/3392175350802835/)

[Meta. (z.d.). Hand Pose Detection. meta.com. Geraadpleegd op 18 november 2024, van https://developers.meta.com/horizon/documentation/unity/unity-isdk-hand-pose-detection#velocityrecognition](https://developers.meta.com/horizon/documentation/unity/unity-isdk-hand-pose-detection#velocityrecognition)

[Schäfer, A., Reis, G., & Stricker, D. (2022). AnyGesture: Arbitrary One-Handed Gestures for Augmented, Virtual, and Mixed Reality Applications. Applied Sciences, 12(4), 1888. https://doi.org/10.3390/app12041888](https://www.mdpi.com/2076-3417/12/4/1888)
